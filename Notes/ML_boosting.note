# Градиентный Бустинг
См тетрадь.
Искомую функцию приближаем интеративно. Т.е. на каждой итерации прибавляем к результирующей функции функцию с весом.
Функцию берем из параметрического множетва, оптимизируя приближение антиградиента ошибки, которую мы получили с помощью результирующей функции(т.е. каждое следующее слагаемое должно обучатсья конпенсировать ошибки предыдущего). 
Вес подбираем уже из минимизации функции потерь при новой результирующей функции. 
* ODS https://habr.com/company/ods/blog/327250/ 
* АМД: https://alexanderdyakonov.files.wordpress.com/2017/06/book_boosting_pdf.pdf
Чаще всего применяют бустинг линейных алгоритмов и бустинг решающих деревьев.
Используют learning rate, чтобы конпенсировать не сразу весь градиент. Так лучше сходится.

## Функции потерь
* Регрессия:
    - Gaussian loss (L_2). Классический, если нет требований по робасности - хороший вариант.
    - Laplacian loss (L_1). Условная медиана. Более устойчива к выбросам
    - Quantile loss (L_q). Как L_1, только ошибки в разных направлениях по разному штрафуются.
    - Huber loss - на небольших отклонениях как L_2, начиная с нек порога как L_1
* Классификация:
    -  Logistic loss, Bernoulli loss. Штрафует даже правильно предсказанные классы.
       За счет этого раздвигает классы.
    - Adaboost loss. Похож на логистическую, но сильной штрафует за неправильные метки.
* Возможно вместо использованяи своей функции потерь добавлять веса классам.(см ODS)

## Реализации:
Основные:  xgboost, lightgbm и h2o

## Параметры
Параметры и схема настройки ГБ Описаны в pdf book_boosting_pdf.pdf