# Решающее Дерево

## Преимущества
 + Интерпретируемые + Визуализируемые
 + Быстрые train и test
 + Мало параметров
 + Числовые и Категоральные признаки
 
## Недостатки
 - Чувствительны к шумам и выборке
 - Разделяющая поверзность - набор гиперплоскостей
 - Необх в pruning или max_depth
 - Сложно с пропусками
 - Интерполирует, но не экстраполирует

## Параметры:
 * **Критерий разбиения**:
    - **Классификация**
        - **Джини**:                           G = 1 - \sum\limits_k (p_k)^2
        - **Прирост Информации(энтропийный)**: S = -\sum_{i=1}^{N}p_i \log_2{p_i}
        Они очень похожи!
    - **Регрессия**
        - **Дисперсия вокруг среднего** - Разбивает так, чтобы значения в каждом листе примерно одинаковые
        D = (1\l)  \sum\limits_{i =1}^{l} (y_i - (1/l) \sum\limits_{i =1}^{l} y_i)^2,    
 * **max_depth** - всегда, если не лес, или Pruning!
 * **max_features** - число пр, по которым ищется разбиение(не орг = долго!). Для   
                      каждого расщепления - свое множество признаков.
 * **min_samples_leaf** - минимальное число объектов в листе. Рек: 1-класс, 5-рег

## Эвристики:
 * Ограничение множества возможных порогов(например, для количественных)
 *



# Random Forest:
Бэггинг над решающими деревьями с методом случайных подпространств(случайное подмножество признаков) - лучше, чем просто бэггинг решающих деревьев.
Уменьшается дисперсия(как в бэггинге)

* ODS https://habr.com/company/ods/blog/324402/
* Анализ малых данных https://dyakonov.org/2016/11/14/
* интерпретация: https://drive.google.com/file/d/164Els1Qhg6uYJyPNWOJF8awoDwrrGSzV/view

## Преимущества:
 + точен, как правило лучше линейных, сравним с бустингом
 + Не чувствителен в выбросам (почему? видимо, случайность семплирования и настрйока глубины)
 + Не чувствителен к почти всем монотонным преобразованиям признаков
 + Непрерывные и дискретные признаки
 + Работает с пропусками
 + редко переобучается (? если данные не зашумлены)
 + эффективный
 + почти универсален
 + можно оценивать важности признаков - в sklearn не очень, зато быстро. (лучше R).
   вот тут есть как правильно: http://docs.salford-systems.com/AdeleCutler.pdf
 + можно считать OOB оценки, могут быть смещенными(почему? может, корреляция? посмотреть реализацию @todo)  
 
## Недостатки:
 - Не очень на задачах с изображениями и большом количестве разреженных признаков(Bag of smth) - как и любые алгоритмы на деревьях. (Стоит начать с лин или сеток.)
 - Относительно плохо интерпретируется
 - для данных, включающих категориальные переменные с различным количеством уровней, случайные леса предвзяты в пользу признаков с большим количеством уровней: когда у признака много уровней, дерево будет сильнее подстраиваться именно под эти признаки, так как на них можно получить более высокое значение оптимизируемого функционала (типа прироста информации) [ODS]
 - если данные содержат группы коррелированных признаков, имеющих схожую значимость для меток, то предпочтение отдается небольшим группам перед большими [ODS]

## Параметры
* Те, что у обычных решающих деревьев
* **n_estimators** - Количество деревьев - > = > качество, но часто выходит на асимптоту.
* **max_features** - очень важный параметр, качество на тесте унимодально(один пик)
* **bootstrap** True - с возвращение, False - без возвращения, как jackknife

## Производные методы:

* Сверхслучайные деревья: ExtraTrees, случайность в выборе расщепления.
* RandomTreesEmbedding - преобразование признаков в многомерное пространство.
  Идея - стоим абсолютно случайные деревья, каждый лист - признак, попал 1, не попал 0.
  Непараметрическая оценка плотности(в одном листе близкие объекты)


 
## TreeBoost